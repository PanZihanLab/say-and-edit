<h1 align="center">Say and Edit: Text-Guided 3D Editing With Automatic Mask and Self-Guidance</h1>

<h2 align="center">Abstract</h2>

<p align="justify">
Text-guided 3D object editing remains challenging due to inaccurate local region localization, insufficient semantic control, and difficulty in maintaining 3D consistency. To address these issues, we propose a diffusion-based two-stage editing framework that decomposes the text-driven 3D editing task into (1) text-guided image editing and (2) image-to-3D reconstruction, enabling stable and high-quality 3D editing results. In the first stage, we perform instruction-aware image editing by decomposing the editing instruction into fine-grained semantic targets, generating accurate editing masks via cross-modal matching, and introducing a self-guidance mechanism to strengthen the semantic direction of the diffusion process, thereby achieving high-precision local editing while preserving structural consistency. In the second stage, we reconstruct a consistent 3D model from the edited image using a volumetric Gaussian representation, where Gaussian primitives are optimized in both geometry and appearance through geometric priors and differentiable rendering, ensuring stable shape and fine details across views. Experimental comparisons with strong baselines such as LRM and InstructPix2Pix (IP2P) demonstrate that our method achieves higher semantic alignment in image editing (CLIP-Score 26.32) with lower perceptual error (LPIPS 0.22) while reducing editing time to 7 seconds. For 3D reconstruction, our approach reaches PSNR 20.04, SSIM 0.785, and reduces FID to 26.38, outperforming existing baselines overall.
</p>
